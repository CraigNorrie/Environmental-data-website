#Create the map - in the future set all of the variables in a csv to be read in
map <- leaflet() %>%
addProviderTiles(providers$Esri.OceanBasemap) %>%
addMarkers(lng = siteinfo$Long, lat = siteinfo$Lat,
popup = lapply(seq(nrow(siteinfo)), function(i) {
HTML(paste0("<strong>", siteinfo$Site_name[i], "</strong><br>",
"Availble data: ", siteinfo$Data[i], "<br>",
"Organisation: ", siteinfo$Organisation[i], "<br>",
"Contact: ", siteinfo$Contact[i]))
}))
View(siteinfo)
library(here)
---
title: "Hood Head environmental data"
# Chunk 1
#Load the packages required to make a local scale map
library(tidyverse)
library(leaflet)#package to make interactive web
library(leaflet.providers)#allows you to add third party basemaps
library(here)
library(plotly)
library(lubridate)
# Chunk 2
aquaculturemap_hood <- leaflet() %>%
addProviderTiles(providers$Esri.OceanBasemap) %>%
addMarkers(lng = -122.612914,lat = 47.884253, popup = "Hood Head - Blue Dot Sea Farm") #Hood Head
# Chunk 3
aquaculturemap_hood
# Chunk 4
#Sumarise the data
#turn this into a loop that will do this for each of the cvs files that I have in the data folder
hooddat <- read.csv(here("Data", "hood_Do_Temp.csv"))#Read in the data
hooddat$datetime <- mdy_hm(hooddat$Date.Time..GMT.07.00)
#Obtain daily mean data - this should probably be done in a seperate script and this should be saved for the website only
hoodav <- hooddat %>%
mutate(date = as.Date(date, format = "%Y-%m-%d")) %>%
group_by(date) %>%
summarise(meantemp = mean(Temp_C, na.rm = TRUE),
meanDO = mean(DO_mgL, na.rm = TRUE))
# Chunk 5
hood_temp_plot <- hoodav %>%
ggplot(aes(x = date, y = meantemp))+geom_line()+xlab("Date")+ylab("Temperature (°C)")+theme_bw()
# Chunk 6
ggplotly(hood_temp_plot)
# Chunk 7
hood_do_plot <- hoodav %>%
ggplot(aes(x = date, y = meanDO))+geom_line()+xlab("Date")+ylab("Disolved oxygen (mg/L)")+theme_bw()
ggplotly(hood_do_plot)
# Chunk 8
ggplotly(hood_do_plot)
hood_do_plot <- hoodav %>%
ggplot(aes(x = date, y = meanDO))+geom_line()+xlab("Date")+ylab("Disolved oxygen (mg/L)")+theme_bw()+geom_blank()
ggplotly(hood_do_plot)
ggplot(aes(x = date, y = meanDO))+geom_line()+xlab("Date")+ylab("Disolved oxygen (mg/L)")+theme_bw()+geom_blank()
ggplot(aes(x = date, y = meanDO))+geom_line()+xlab("Date")+ylab("Disolved oxygen (mg/L)")+theme_bw()
aquaculturemap_hood <- leaflet() %>%
addProviderTiles(providers$Esri.OceanBasemap) %>%
addMarkers(lng = -122.612914,lat = 47.884253, popup = "Hood Head - Blue Dot Sea Farm") #Hood Head
aquaculturemap_hood
#Sumarise the data
#turn this into a loop that will do this for each of the cvs files that I have in the data folder
hooddat <- read.csv(here("Data", "hood_Do_Temp.csv"))#Read in the data
hooddat$datetime <- mdy_hm(hooddat$Date.Time..GMT.07.00)
#Obtain daily mean data - this should probably be done in a seperate script and this should be saved for the website only
hoodav <- hooddat %>%
mutate(date = as.Date(date, format = "%Y-%m-%d")) %>%
group_by(date) %>%
summarise(meantemp = mean(Temp_C, na.rm = TRUE),
meanDO = mean(DO_mgL, na.rm = TRUE))
hood_temp_plot <- hoodav %>%
ggplot(aes(x = date, y = meantemp))+geom_line()+xlab("Date")+ylab("Temperature (°C)")+theme_bw()
ggplotly(hood_temp_plot)
hood_do_plot <- hoodav %>%
ggplot(aes(x = date, y = meanDO))+geom_line()+xlab("Date")+ylab("Disolved oxygen (mg/L)")+theme_bw()#+geom_blank()
ggplotly(hood_do_plot)
ggplotly(hood_do_plot)
#Sumarise the data
#turn this into a loop that will do this for each of the cvs files that I have in the data folder
hooddat <- read.csv(here("Data", "hood_Do_Temp.csv"))#Read in the data
hooddat$datetime <- mdy_hm(hooddat$Date.Time..GMT.07.00)
#Obtain daily mean data - this should probably be done in a seperate script and this should be saved for the website only
hoodav <- hooddat %>%
mutate(date = as.Date(date, format = "%Y-%m-%d")) %>%
group_by(date) %>%
summarise(meantemp = mean(Temp_C, na.rm = TRUE),
meanDO = mean(DO_mgL, na.rm = TRUE))
hood_temp_plot <- hoodav %>%
ggplot(aes(x = date, y = meantemp))+geom_line()+xlab("Date")+ylab("Temperature (°C)")+theme_bw()
ggplotly(hood_temp_plot)
library(tidyverse)
library(leaflet)#package to make interactive web
library(leaflet.providers)#allows you to add third party basemaps
library(here)
library(plotly)
library(lubridate)
mandat <- read.csv(here("Data", "man_Do_Temp.csv"))#Read in the data
mandat$datetime <- mdy_hm(mandat$Date.Time..GMT.07.00)
#Obtain daily mean data - this should probably be done in a seperate script and this should be saved for the website only
manav <- mandat %>%
mutate(date = as.Date(date, format = "%Y-%m-%d")) %>%
group_by(date) %>%
summarise(meantemp = mean(Temp_C, na.rm = TRUE),
meanDO = mean(DO_mgL, na.rm = TRUE))
man_temp_plot <- manav %>%
ggplot(aes(x = date, y = meantemp))+geom_line()+xlab("Date")+ylab("Temperature (°C)")+theme_bw()
man_temp_plot
mandat <- read.csv(here("Data", "man_Do_Temp.csv"))#Read in the data
mandat$datetime <- mdy_hm(mandat$Date.Time..GMT.07.00)
#Obtain daily mean data - this should probably be done in a seperate script and this should be saved for the website only
manav <- mandat %>%
mutate(date = as.Date(date, format = "%Y-%m-%d")) %>%
group_by(date) %>%
summarise(meantemp = mean(Temp_C, na.rm = TRUE),
meanDO = mean(DO_mgL, na.rm = TRUE))
man_temp_plot <- manav %>%
ggplot(aes(x = date, y = meantemp))+geom_line()+xlab("Date")+ylab("Temperature (°C)")+theme_bw()
man_temp_plot
#Sumarise the data
#turn this into a loop that will do this for each of the cvs files that I have in the data folder
mandat <- read.csv(here("Data", "man_Do_Temp.csv"))#Read in the data
mandat$datetime <- mdy_hm(mandat$Date.Time..GMT.07.00)
#Obtain daily mean data - this should probably be done in a seperate script and this should be saved for the website only
manav <- mandat %>%
mutate(date = as.Date(date, format = "%Y-%m-%d")) %>%
group_by(date) %>%
summarise(meantemp = mean(Temp_C, na.rm = TRUE),
meanDO = mean(DO_mgL, na.rm = TRUE))
man_temp_plot <- manav %>%
ggplot(aes(x = date, y = meantemp))+geom_line()+xlab("Date")+ylab("Temperature (°C)")+theme_bw()
man_temp_plot
mandat <- read.csv(here("Data", "man_Do_Temp.csv"))#Read in the data
mandat$datetime <- mdy_hm(mandat$Date.Time..GMT.07.00)
#Obtain daily mean data - this should probably be done in a seperate script and this should be saved for the website only
manav <- mandat %>%
mutate(date = as.Date(date, format = "%Y-%m-%d")) %>%
group_by(date) %>%
summarise(meantemp = mean(Temp_C, na.rm = TRUE),
meanDO = mean(DO_mgL, na.rm = TRUE))
man_temp_plot <- manav %>%
ggplot(aes(x = date, y = meantemp))+geom_line()+xlab("Date")+ylab("Temperature (°C)")+theme_bw()
man_temp_plot
ggplotly(man_do_plot)
ggplotly(man_temp_plot)
ggplotly(man_temp_plot)
man_temp_plot
---
title: "Chelsea environmental data"
# Chunk 1
#Load the packages required to make a local scale map
library(tidyverse)
library(leaflet)#package to make interactive web
library(leaflet.providers)#allows you to add third party basemaps
library(here)
library(plotly)
library(lubridate)
# Chunk 2
aquaculturemap_chel <- leaflet() %>%
addProviderTiles(providers$Esri.OceanBasemap) %>%
addMarkers(lng = -122.960441,lat = 47.127609, popup = "South Sound - Chelsea Farms")
# Chunk 3
aquaculturemap_chel
# Chunk 4
#Sumarise the data
#turn this into a loop that will do this for each of the cvs files that I have in the data folder
cheldat <- read.csv(here("Data", "chelsea_Do_Temp.csv"))#Read in the data
cheldat$datetime <- mdy_hm(cheldat$Date.Time..GMT.07.00)
#Obtain daily mean data - this should probably be done in a separate script and this should be saved for the website only
chelav <- cheldat %>%
mutate(date = as.Date(date, format = "%Y-%m-%d")) %>%
group_by(date) %>%
summarise(meantemp = mean(Temp_C, na.rm = TRUE),
meanDO = mean(DO_mgL, na.rm = TRUE))
# Chunk 5
chel_temp_plot <- chelav %>%
ggplot(aes(x = date, y = meantemp))+geom_line()+xlab("Date")+ylab("Temperature (°C)")+theme_bw()
# Chunk 6
ggplotly(chel_temp_plot)
# Chunk 7
chel_do_plot <- chelav %>%
ggplot(aes(x = date, y = meanDO))+geom_line()+xlab("Date")+ylab("Disolved oxygen (mg/L)")+theme_bw()
ggplotly(chel_do_plot)
# Chunk 8
ggplotly(chel_do_plot)
###
experiment_name <- "Multistressor_2024"
# Load packages ------------------------------------------------------------
library(tidyverse)
library(gridExtra)
library(lubridate)
library(respR) # I had to install this from github devtools::install_github("januarharianto/respR")
library(here)
# Read in all the data ----------------------------------------------------
rawfiles <- list.files(here("Raw_data"),
all.files = FALSE) #Creates a vector with all the filenames from respirometry that are stored in the "raw_data" folder.
respdat_all <- data.frame()#Creates an empty data frame to store respirometry data
#The for loop here will read in each of the data files
for(i in 1:length(rawfiles)){
tempdat <- read.csv(file = here("Raw_data", rawfiles[i]), skip = 1)
tempdat <- slice(tempdat, 1:(n() - 2))#removes the last two rows becasue it is junk
tempdat$run <- i#add a new column based on the number sheet that it is. i.e. run 1, run 2. **important that sheets are named in order of resplog**
respdat_all <- rbind(respdat_all, tempdat)#used plyr::rbind instead of dplyr::bind_rows because this deals with duplicate column names
rm(tempdat)
}
respdat_all
# Read in all the data ----------------------------------------------------
rawfiles <- list.files(here("Raw_data"),
all.files = FALSE) #Creates a vector with all the filenames from respirometry that are stored in the "raw_data" folder.
rawfiles
###
experiment_name <- "Multistressor_2024"
here()
View(allthorndat)
View(thorndat)
thorndat <- read.csv(here("Data", "thorn_Do_Temp.csv"))#Read in the data
#Load the packages required to make a local scale map
library(tidyverse)
library(leaflet)#package to make interactive web
library(leaflet.providers)#allows you to add third party basemaps
library(here)
library(plotly)
library(lubridate)
#Sumarise the data
#turn this into a loop that will do this for each of the cvs files that I have in the data folder
thorndat <- read.csv(here("Data", "thorn_Do_Temp.csv"))#Read in the data
View(thorndat)
# Chunk 1
#Load the packages required for this
library(tidyverse)
library(leaflet)#package to make interactive web
library(leaflet.providers)#allows you to add third party basemaps
library(here)
library(plotly)
library(htmltools)
# Chunk 2
###Create the map
#Read in site info
siteinfo <- read.csv(here("Site_info.csv"))
#Create the map - in the future set all of the variables in a csv to be read in
aquaculturemap <- leaflet() %>%
addProviderTiles(providers$Esri.OceanBasemap) %>%
addMarkers(lng = siteinfo$Long, lat = siteinfo$Lat,
popup = lapply(seq(nrow(siteinfo)), function(i) {
HTML(paste0("<strong>", siteinfo$Site_name[i], "</strong><br>",
"Availble data: ", siteinfo$Data[i], "<br>",
"Organisation: ", siteinfo$Organisation[i], "<br>",
"Contact: ", siteinfo$Contact[i]))
}))
# Chunk 3
aquaculturemap
# Chunk 4
#Create averages
##read in the data
allthorndat <- read.csv(here("Data","thorn_Do_Temp.csv"))
allcheldat <- read.csv(here("Data","chelsea_Do_Temp.csv"))
allhooddat <- read.csv(here("Data","hood_Do_Temp.csv"))
allmandat <- read.csv(here("Data","man_Do_Temp.csv"))
#Set the first column as date
allthorndat$datetime <- mdy_hm(allthorndat$Date.Time..GMT.07.00)
allcheldat$datetime  <- mdy_hm(allcheldat$Date.Time..GMT.07.00)
allhooddat$datetime  <- mdy_hm(allhooddat$Date.Time..GMT.07.00)
allmandat$datetime  <- mdy_hm(allmandat$Date.Time..GMT.07.00)
#add a site columin to each of the DFs
allcheldat <- allcheldat %>% mutate(Site = "Chelsea")
allthorndat <- allthorndat %>% mutate(Site = "Thorndyke")
allhooddat <- allhooddat %>% mutate(Site = "Hood_Head")
allmandat <- allmandat %>% mutate(Site = "Manchester")
#add a date column to the two dfs
allcheldat$date <- as.Date(allcheldat$datetime)
allthorndat$date <- as.Date(allthorndat$datetime)
allhooddat$date <- as.Date(allhooddat$datetime)
allmandat$date <- as.Date(allmandat$datetime)
#calculate averages
chelav <- allcheldat %>% group_by(date) %>% summarise(meantemp=mean(Temp_C, na.rm = TRUE), meanDO=mean(DO_mgL, na.rm = TRUE))
thornav <- allthorndat %>% group_by(date) %>% summarise(meantemp=mean(Temp_C, na.rm = TRUE), meanDO=mean(DO_mgL, na.rm = TRUE))
manav <- allmandat %>% group_by(date) %>% summarise(meantemp=mean(Temp_C, na.rm = TRUE), meanDO=mean(DO_mgL, na.rm = TRUE))
hoodav <- allhooddat %>% group_by(date) %>% summarise(meantemp=mean(Temp_C, na.rm = TRUE), meanDO=mean(DO_mgL, na.rm = TRUE))
#Only caclulate the days I want
chelav <- chelav %>% filter(date > "2023-06-05")
thornav <- thornav %>% filter(date > "2023-06-05")
hoodav <- hoodav %>% filter(date > "2023-06-05")
manav <- manav %>% filter(date > "2023-06-05")
#calculate cumulative degree days
chelav$degdays <- cumsum(chelav$meantemp)
thornav$degdays <- cumsum(thornav$meantemp)
#Add site to the average dfs
chelav <- chelav %>% mutate(Site = "Chelsea")
thornav <- thornav %>% mutate(Site = "Thorndyke")
manlav <- manav %>% mutate(Site = "Manchester")
hoodav <- hoodav %>% mutate(Site = "Hood_Head")
#merge the two sites temperature data to plot
alldat <- bind_rows(allcheldat, allthorndat, allmandat, allhooddat)
allavdat <- bind_rows(chelav, thornav, hoodav, manav)
dailyaverages <- alldat
alldat$date <- as.Date(alldat$datetime)
averagetemps <- alldat %>% group_by(date, Site) %>% summarise(meantemp=mean(Temp_C, na.rm = TRUE))
averageDO <- alldat %>% group_by(date, Site) %>% summarise(meanDO=mean(DO_mgL, na.rm = TRUE))
#Plot DO
avergage_DO_plot <- averageDO %>%
filter(date > "2023-06-05") %>% #filter(date < "2023-10-17") %>%
ggplot(aes(x= date , y = meanDO, colour = Site)) + geom_line()+
theme_bw()+ylab("Daily Mean Dissolved Oxygen (mg / L)")+xlab("Date")+
scale_color_manual(values=c("#B3C863","#756A8C","#037D64","#CEB08E"))+
theme(axis.title.y=element_text())+
theme(
text = element_text(size = 14))
# Chunk 5
ggplotly(avergage_DO_plot)
# Chunk 6
averagetemps <- averagetemps %>%
filter(date > "2023-06-05") %>% #filter(date < "2023-10-17") %>%
ggplot(aes(x= date , y = meantemp, colour = Site)) + geom_line(size=1)+
theme_bw()+ylab("Daily Mean Temperature (°C)")+xlab("Date")+
scale_color_manual(values=c("#B3C863","#756A8C","#037D64","#CEB08E"))+
theme(text=element_text(),
axis.title.y=element_text())
# Chunk 7
ggplotly(averagetemps)
averagetemps <- averagetemps %>%
filter(date > "2023-06-05") %>% #filter(date < "2023-10-17") %>%
ggplot(aes(x= date , y = meantemp, colour = Site)) + geom_line(size=1)+
theme_bw()+ylab("Daily Mean Temperature (°C)")+xlab("Date")+
scale_color_manual(values=c("#B3C863","#756A8C","#037D64","#CEB08E"))+
theme(text=element_text(),
axis.title.y=element_text())
avergage_DO_plot <- averageDO %>%
filter(date > "2023-06-05") %>% #filter(date < "2023-10-17") %>%
ggplot(aes(x= date , y = meanDO, colour = Site)) + geom_line()+
theme_bw()+ylab("Daily Mean Dissolved Oxygen (mg / L)")+xlab("Date")+
scale_color_manual(values=c("#B3C863","#756A8C","#037D64","#CEB08E"))+
theme(axis.title.y=element_text())+
theme(
text = element_text(size = 14))
averagetemps <- averagetemps %>%
filter(date > "2023-06-05") %>% #filter(date < "2023-10-17") %>%
ggplot(aes(x= date , y = meantemp, colour = Site)) + geom_line(size=1)+
theme_bw()+ylab("Daily Mean Temperature (°C)")+xlab("Date")+
scale_color_manual(values=c("#B3C863","#756A8C","#037D64","#CEB08E"))+
theme(text=element_text(),
axis.title.y=element_text())
averagetemps <- alldat %>% group_by(date, Site) %>% summarise(meantemp=mean(Temp_C, na.rm = TRUE))
View(averagetemps)
averagetemps <- averagetemps %>%
filter(date > "2023-06-05") %>% #filter(date < "2023-10-17") %>%
ggplot(aes(x= date , y = meantemp, colour = Site)) + geom_line(size=1)+
theme_bw()+ylab("Daily Mean Temperature (°C)")+xlab("Date")+
scale_color_manual(values=c("#B3C863","#756A8C","#037D64","#CEB08E"))+
theme(text=element_text(),
axis.title.y=element_text())
# Chunk 1
#Load the packages required for this
library(tidyverse)
library(leaflet)#package to make interactive web
library(leaflet.providers)#allows you to add third party basemaps
library(here)
library(plotly)
library(htmltools)
# Chunk 2
###Create the map
#Read in site info
siteinfo <- read.csv(here("Site_info.csv"))
#Create the map - in the future set all of the variables in a csv to be read in
aquaculturemap <- leaflet() %>%
addProviderTiles(providers$Esri.OceanBasemap) %>%
addMarkers(lng = siteinfo$Long, lat = siteinfo$Lat,
popup = lapply(seq(nrow(siteinfo)), function(i) {
HTML(paste0("<strong>", siteinfo$Site_name[i], "</strong><br>",
"Availble data: ", siteinfo$Data[i], "<br>",
"Organisation: ", siteinfo$Organisation[i], "<br>",
"Contact: ", siteinfo$Contact[i]))
}))
# Chunk 3
aquaculturemap
# Chunk 4
#Create averages
##read in the data
allthorndat <- read.csv(here("Data","thorn_Do_Temp.csv"))
allcheldat <- read.csv(here("Data","chelsea_Do_Temp.csv"))
allhooddat <- read.csv(here("Data","hood_Do_Temp.csv"))
allmandat <- read.csv(here("Data","man_Do_Temp.csv"))
#Set the first column as date
allthorndat$datetime <- mdy_hm(allthorndat$Date.Time..GMT.07.00)
allcheldat$datetime  <- mdy_hm(allcheldat$Date.Time..GMT.07.00)
allhooddat$datetime  <- mdy_hm(allhooddat$Date.Time..GMT.07.00)
allmandat$datetime  <- mdy_hm(allmandat$Date.Time..GMT.07.00)
#add a site columin to each of the DFs
allcheldat <- allcheldat %>% mutate(Site = "Chelsea")
allthorndat <- allthorndat %>% mutate(Site = "Thorndyke")
allhooddat <- allhooddat %>% mutate(Site = "Hood_Head")
allmandat <- allmandat %>% mutate(Site = "Manchester")
#add a date column to the two dfs
allcheldat$date <- as.Date(allcheldat$datetime)
allthorndat$date <- as.Date(allthorndat$datetime)
allhooddat$date <- as.Date(allhooddat$datetime)
allmandat$date <- as.Date(allmandat$datetime)
#calculate averages
chelav <- allcheldat %>% group_by(date) %>% summarise(meantemp=mean(Temp_C, na.rm = TRUE), meanDO=mean(DO_mgL, na.rm = TRUE))
thornav <- allthorndat %>% group_by(date) %>% summarise(meantemp=mean(Temp_C, na.rm = TRUE), meanDO=mean(DO_mgL, na.rm = TRUE))
manav <- allmandat %>% group_by(date) %>% summarise(meantemp=mean(Temp_C, na.rm = TRUE), meanDO=mean(DO_mgL, na.rm = TRUE))
hoodav <- allhooddat %>% group_by(date) %>% summarise(meantemp=mean(Temp_C, na.rm = TRUE), meanDO=mean(DO_mgL, na.rm = TRUE))
#Only caclulate the days I want
chelav <- chelav %>% filter(date > "2023-06-05")
thornav <- thornav %>% filter(date > "2023-06-05")
hoodav <- hoodav %>% filter(date > "2023-06-05")
manav <- manav %>% filter(date > "2023-06-05")
#calculate cumulative degree days
chelav$degdays <- cumsum(chelav$meantemp)
thornav$degdays <- cumsum(thornav$meantemp)
#Add site to the average dfs
chelav <- chelav %>% mutate(Site = "Chelsea")
thornav <- thornav %>% mutate(Site = "Thorndyke")
manlav <- manav %>% mutate(Site = "Manchester")
hoodav <- hoodav %>% mutate(Site = "Hood_Head")
#merge the two sites temperature data to plot
alldat <- bind_rows(allcheldat, allthorndat, allmandat, allhooddat)
allavdat <- bind_rows(chelav, thornav, hoodav, manav)
dailyaverages <- alldat
alldat$date <- as.Date(alldat$datetime)
averagetemps <- alldat %>% group_by(date, Site) %>% summarise(meantemp=mean(Temp_C, na.rm = TRUE))
averageDO <- alldat %>% group_by(date, Site) %>% summarise(meanDO=mean(DO_mgL, na.rm = TRUE))
#Plot DO
avergage_DO_plot <- averageDO %>%
filter(date > "2023-06-05") %>% #filter(date < "2023-10-17") %>%
ggplot(aes(x= date , y = meanDO, colour = Site)) + geom_line()+
theme_bw()+ylab("Daily Mean Dissolved Oxygen (mg / L)")+xlab("Date")+
scale_color_manual(values=c("#B3C863","#756A8C","#037D64","#CEB08E"))+
theme(axis.title.y=element_text())+
theme(
text = element_text(size = 14))
# Chunk 5
ggplotly(avergage_DO_plot)
# Chunk 6
averagetemps <- averagetemps %>%
filter(date > "2023-06-05") %>% #filter(date < "2023-10-17") %>%
ggplot(aes(x= date , y = meantemp, colour = Site)) + geom_line(size=1)+
theme_bw()+ylab("Daily Mean Temperature (°C)")+xlab("Date")+
scale_color_manual(values=c("#B3C863","#756A8C","#037D64","#CEB08E"))+
theme(text=element_text(),
axis.title.y=element_text())
# Chunk 7
ggplotly(averagetemps)
View(hooddat)
View(cheldat)
View(cheldat)
#Sumarise the data
#turn this into a loop that will do this for each of the cvs files that I have in the data folder
cheldat <- read.csv(here("Data", "chelsea_Do_Temp.csv"))#Read in the data
cheldat$datetime <- mdy_hm(cheldat$Date.Time..GMT.07.00)
library(tidyverse)
library(leaflet)#package to make interactive web
library(leaflet.providers)#allows you to add third party basemaps
library(here)
library(plotly)
library(lubridate)
#Sumarise the data
#turn this into a loop that will do this for each of the cvs files that I have in the data folder
cheldat <- read.csv(here("Data", "chelsea_Do_Temp.csv"))#Read in the data
cheldat$datetime <- mdy_hm(cheldat$Date.Time..GMT.07.00)
#Obtain daily mean data - this should probably be done in a separate script and this should be saved for the website only
chelav <- cheldat %>%
mutate(date = as.Date(date, format = "%Y-%m-%d")) %>%
group_by(date) %>%
summarise(meantemp = mean(Temp_C, na.rm = TRUE),
meanDO = mean(DO_mgL, na.rm = TRUE))
#Sumarise the data
#turn this into a loop that will do this for each of the cvs files that I have in the data folder
cheldat <- read.csv(here("Data", "chelsea_Do_Temp.csv"))#Read in the data
cheldat$datetime <- mdy_hm(cheldat$Date.Time..GMT.07.00)
#Obtain daily mean data - this should probably be done in a separate script and this should be saved for the website only
chelav <- cheldat %>%
mutate(date = as.Date(date, format = "%Y-%m-%d")) %>%
group_by(date) %>%
summarise(meantemp = mean(Temp_C, na.rm = TRUE),
meanDO = mean(DO_mgL, na.rm = TRUE))
chel_temp_plot <- chelav %>%
ggplot(aes(x = date, y = meantemp))+geom_line()+xlab("Date")+ylab("Temperature (°C)")+theme_bw()
ggplotly(chel_temp_plot)
# Chunk 1
#Load the packages required to make a local scale map
library(tidyverse)
library(leaflet)#package to make interactive web
library(leaflet.providers)#allows you to add third party basemaps
library(here)
library(plotly)
library(lubridate)
# Chunk 2
aquaculturemap_chel <- leaflet() %>%
addProviderTiles(providers$Esri.OceanBasemap) %>%
addMarkers(lng = -122.960441,lat = 47.127609, popup = "South Sound - Chelsea Farms")
# Chunk 3
aquaculturemap_chel
# Chunk 4
#Sumarise the data
#turn this into a loop that will do this for each of the cvs files that I have in the data folder
cheldat <- read.csv(here("Data", "chelsea_Do_Temp.csv"))#Read in the data
cheldat$datetime <- mdy_hm(cheldat$Date.Time..GMT.07.00)
#Obtain daily mean data - this should probably be done in a separate script and this should be saved for the website only
chelav <- cheldat %>%
mutate(date = as.Date(date, format = "%Y-%m-%d")) %>%
group_by(date) %>%
summarise(meantemp = mean(Temp_C, na.rm = TRUE),
meanDO = mean(DO_mgL, na.rm = TRUE))
# Chunk 5
chel_temp_plot <- chelav %>%
ggplot(aes(x = date, y = meantemp))+geom_line()+xlab("Date")+ylab("Temperature (°C)")+theme_bw()
# Chunk 6
ggplotly(chel_temp_plot)
# Chunk 7
chel_do_plot <- chelav %>%
ggplot(aes(x = date, y = meanDO))+geom_line()+xlab("Date")+ylab("Disolved oxygen (mg/L)")+theme_bw()
ggplotly(chel_do_plot)
# Chunk 8
ggplotly(chel_do_plot)
