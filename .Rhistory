quarto render
# Locations within thorndyke bay for sensors
```{r, echo = FALSE, results = "hide"}
#Load the packages required to make a local scale map
library(tidyverse)
library(leaflet)#package to make interactive web
library(leaflet.providers)#allows you to add third party basemaps
###Create the map
#Set the content for each point on the map
content_thorndyke_UW <- paste("<b>Thorndyke Bay, Baywater Shellfish - UW</b><br/>
Data: Disolved oxygen, temperature, salinity<br/>
Contact: Craig Norrie")
content_thorndyke_PSI <- paste("<b>Thorndyke Bay, Baywater Shellfish - UW</b><br/>
Data: data link<br/>
Contact: PSI")
content_thorndyke_Seaweed <- paste("<b>Thorndyke Bay, Baywater Shellfish - UW</b><br/>
Data: Seaweed<br/>
Contact: Seaweed")
#Create the map - in the future set all of the variables in a csv to be read in
aquaculturemap_thorn <- leaflet() %>%
addProviderTiles(providers$Esri.OceanBasemap) %>%
addMarkers(lng = -122.738067,lat = 47.808739, popup = content_thorndyke_UW) %>% #UW
addMarkers(lng = -122.735507,lat = 47.809411, popup = content_thorndyke_PSI) %>% #PSI
addMarkers(lng = -122.734758,lat = 47.809131, popup = content_thorndyke_Seaweed) #Seaweed
aquaculturemap_thorn
here()
library()here
library(here)
here()
thorndat <- read.csv(here("Data", "thorn_Do_Temp.csv"))#Read in the data
View(thorndat)
Thorn_do_plot <- thorndat %>%
ggplot(aes(x = Date.Time..GMT.07.00, y = DO_mgL)+geom_line())
thorndat <- read.csv(here("Data", "thorn_Do_Temp.csv"))#Read in the data
Thorn_do_plot <- thorndat %>%
ggplot(aes(x = Date.Time..GMT.07.00, y = DO_mgL)+geom_line())
str(thorndat)
thorndat <- as.Date(thorndat$Date.Time..GMT.07.00)
thorndat <- as.Date(thorndat$Date.Time..GMT.07.00)
thorndat$Date.Time..GMT.07.00 <- as.Date(thorndat$Date.Time..GMT.07.00)
thorndat <- read.csv(here("Data", "thorn_Do_Temp.csv"))#Read in the data
thorndat$datetime <- mdy_hm(allthorndat$Date.Time..GMT.07.00)
thorndat <- read.csv(here("Data", "thorn_Do_Temp.csv"))#Read in the data
thorndat$datetime <- mdy_hm(thorndat$Date.Time..GMT.07.00)
Thorn_do_plot <- thorndat %>%
ggplot(aes(x = Date.Time..GMT.07.00, y = DO_mgL)+geom_line())
thorndat <- read.csv(here("Data", "thorn_Do_Temp.csv"))#Read in the data
thorndat$datetime <- mdy_hm(thorndat$Date.Time..GMT.07.00)
Thorn_do_plot <- thorndat %>%
ggplot(aes(x = datetime, y = DO_mgL)+geom_line())
thorndat <- read.csv(here("Data", "thorn_Do_Temp.csv"))#Read in the data
thorndat$datetime <- mdy_hm(thorndat$Date.Time..GMT.07.00)
Thorn_do_plot <- thorndat %>%
ggplot(aes(x = datetime, y = DO_mgL))+geom_line()
thorndat <- read.csv(here("Data", "thorn_Do_Temp.csv"))#Read in the data
thorndat$datetime <- mdy_hm(thorndat$Date.Time..GMT.07.00)
Thorn_do_plot <- thorndat %>%
ggplot(aes(x = datetime, y = DO_mgL))+geom_line()
Thorn_do_plot
View(thorndat)
#Obtain daily mean data - this should probably be done in a seperate script and this should be saved for the website only
thornav <- thorndat %>% group_by(date) %>% summarise(meantemp=mean(Temp_C, na.rm = TRUE), meanDO=mean(DO_mgL, na.rm = TRUE))
#turn this into a loop that will do this for each of the cvs files that I have in the data folder
thorndat <- read.csv(here("Data", "thorn_Do_Temp.csv"))#Read in the data
thorndat$datetime <- mdy_hm(thorndat$Date.Time..GMT.07.00)
#Obtain daily mean data - this should probably be done in a seperate script and this should be saved for the website only
thornav <- thorndat %>% group_by(date) %>% summarise(meantemp=mean(Temp_C, na.rm = TRUE), meanDO=mean(DO_mgL, na.rm = TRUE))
Thorn_do_plot <- thornav %>%
ggplot(aes(x = date, y = DO_mgL))+geom_line()
Thorn_do_plot
#Obtain daily mean data - this should probably be done in a seperate script and this should be saved for the website only
thornav <- thorndat %>% group_by(date) %>% summarise(meanDO=mean(DO_mgL, na.rm = TRUE), meanDO=mean(DO_mgL, na.rm = TRUE))
Thorn_do_plot <- thornav %>%
ggplot(aes(x = date, y = meanDO))+geom_line()
View(thornav)
#turn this into a loop that will do this for each of the cvs files that I have in the data folder
thorndat <- read.csv(here("Data", "thorn_Do_Temp.csv"))#Read in the data
thorndat$datetime <- mdy_hm(thorndat$Date.Time..GMT.07.00)
#Obtain daily mean data - this should probably be done in a seperate script and this should be saved for the website only
thornav <- thorndat %>% group_by(date) %>% summarise(meantemp=mean(Temp_C, na.rm = TRUE), meanDO=mean(DO_mgL, na.rm = TRUE))
Thorn_do_plot <- thornav %>%
ggplot(aes(x = date, y = meanDO))+geom_line()
Thorn_do_plot
View(thornav)
str(thornav)
#turn this into a loop that will do this for each of the cvs files that I have in the data folder
thorndat <- read.csv(here("Data", "thorn_Do_Temp.csv"))#Read in the data
thorndat$datetime <- mdy_hm(thorndat$Date.Time..GMT.07.00)
#Obtain daily mean data - this should probably be done in a seperate script and this should be saved for the website only
thornav <- thorndat %>%
mutate(date = as.Date(date, format = "%Y-%m-%d")) %>%
group_by(date) %>%
summarise(meantemp = mean(Temp_C, na.rm = TRUE),
meanDO = mean(DO_mgL, na.rm = TRUE))
Thorn_do_plot <- thornav %>%
ggplot(aes(x = date, y = meanDO))+geom_line()
Thorn_do_plot
install.packages("plotly")
install.packages("Rtools")
library(sourcetools)
library(Rtools)
install.packages("Rtools")
version()
R.version
R.version
R.version
#turn this into a loop that will do this for each of the cvs files that I have in the data folder
thorndat <- read.csv(here("Data", "thorn_Do_Temp.csv"))#Read in the data
#Load the packages required to make a local scale map
library(tidyverse)
library(leaflet)#package to make interactive web
library(leaflet.providers)#allows you to add third party basemaps
library(here)
library(plotly)
install.packages("plotly")
install.packages(c("tidyverse", "leaflet", "here", "plotly", "leaflet.providers"))
library(tidyverse)
install.packages("tidyverse")
#Load the packages required to make a local scale map
library(tidyverse)
#Load the packages required for this
library(tidyverse)
install.packages("tidyverse")
library(leaflet)#package to make interactive web
library(leaflet.providers)#allows you to add third party basemaps
#Load the packages required for this
library(tidyverse)
install.packages("tidyverse")
#Load the packages required for this
library(tidyverse)
install.packages("tidyverse")
install.packages("Rtools")
install.packages("tidyverse")
#Load the packages required to make a local scale map
library(tidyverse)
#Load the packages required to make a local scale map
library(tidyverse)
install.packages("tidyverse")
#Load the packages required to make a local scale map
library(tidyverse)
install.packages("gtable")
#Load the packages required to make a local scale map
library(tidyverse)
install.packages("munsell")
#Load the packages required to make a local scale map
library(tidyverse)
install.packages("utf8")
#Load the packages required to make a local scale map
library(tidyverse)
install.packages("tzdb")
#Load the packages required to make a local scale map
library(tidyverse)
library(leaflet)#package to make interactive web
library(leaflet.providers)#allows you to add third party basemaps
library(here)
library(plotly)
ggplotly(Thorn_do_plot)
#turn this into a loop that will do this for each of the cvs files that I have in the data folder
thorndat <- read.csv(here("Data", "thorn_Do_Temp.csv"))#Read in the data
thorndat$datetime <- mdy_hm(thorndat$Date.Time..GMT.07.00)
#Obtain daily mean data - this should probably be done in a seperate script and this should be saved for the website only
thornav <- thorndat %>%
mutate(date = as.Date(date, format = "%Y-%m-%d")) %>%
group_by(date) %>%
summarise(meantemp = mean(Temp_C, na.rm = TRUE),
meanDO = mean(DO_mgL, na.rm = TRUE))
#Obtain daily mean data - this should probably be done in a seperate script and this should be saved for the website only
thornav <- thorndat %>%
mutate(date = as.Date(date, format = "%Y-%m-%d")) %>%
group_by(date) %>%
summarise(meantemp = mean(Temp_C, na.rm = TRUE),
meanDO = mean(DO_mgL, na.rm = TRUE))
thorndat$datetime <- mdy_hm(thorndat$Date.Time..GMT.07.00)
install.packages("lubridate")
thorndat$datetime <- mdy_hm(thorndat$Date.Time..GMT.07.00)
mdy_hm
??mdy_hm
library(lubridate)
thorndat$datetime <- mdy_hm(thorndat$Date.Time..GMT.07.00)
#Obtain daily mean data - this should probably be done in a seperate script and this should be saved for the website only
thornav <- thorndat %>%
mutate(date = as.Date(date, format = "%Y-%m-%d")) %>%
group_by(date) %>%
summarise(meantemp = mean(Temp_C, na.rm = TRUE),
meanDO = mean(DO_mgL, na.rm = TRUE))
Thorn_do_plot <- thornav %>%
ggplot(aes(x = date, y = meanDO))+geom_line()+xlab("Date")+ylab("Disolved oxygen (mg/L)")
Thorn_do_plot
ggplotly(Thorn_do_plot)
ggplotly(Thorn_temp_plot)
#Load the packages required to make a local scale map
library(tidyverse)
library(leaflet)#package to make interactive web
library(leaflet.providers)#allows you to add third party basemaps
library(here)
library(plotly)
library(lubridate)
library(tidyverse)
library(leaflet)#package to make interactive web
library(leaflet.providers)#allows you to add third party basemaps
library(here)
library(plotly)
library(lubridate)
#Sumarise the data
#turn this into a loop that will do this for each of the cvs files that I have in the data folder
cheldat <- read.csv(here("Data", "chel_Do_Temp.csv"))#Read in the data
#Sumarise the data
#turn this into a loop that will do this for each of the cvs files that I have in the data folder
cheldat <- read.csv(here("Data", "chelsea_Do_Temp.csv"))#Read in the data
cheldat$datetime <- mdy_hm(cheldat$Date.Time..GMT.07.00)
#Obtain daily mean data - this should probably be done in a seperate script and this should be saved for the website only
chelav <- cheldat %>%
mutate(date = as.Date(date, format = "%Y-%m-%d")) %>%
group_by(date) %>%
summarise(meantemp = mean(Temp_C, na.rm = TRUE),
meanDO = mean(DO_mgL, na.rm = TRUE))
#Sumarise the data
#turn this into a loop that will do this for each of the cvs files that I have in the data folder
cheldat <- read.csv(here("Data", "chelsea_Do_Temp.csv"))#Read in the data
cheldat$datetime <- mdy_hm(cheldat$Date.Time..GMT.07.00)
#Obtain daily mean data - this should probably be done in a seperate script and this should be saved for the website only
chelav <- cheldat %>%
mutate(date = as.Date(date, format = "%Y-%m-%d")) %>%
group_by(date) %>%
summarise(meantemp = mean(Temp_C, na.rm = TRUE),
meanDO = mean(DO_mgL, na.rm = TRUE))
chel_temp_plot <- chelav %>%
ggplot(aes(x = date, y = meantemp))+geom_line()+xlab("Date")+ylab("Temperature (째C)")+theme_bw()
ggplotly(chel_temp_plot)
copy NUL .nojekyll
#Create averages
##read in the data
#Thorndyke
thorndat_junjuly <- read.csv(here("Data", "Thorndyke_5_june_to_2_aug_2023.csv"))
library(here)
#Load the packages required for this
library(tidyverse)
library(leaflet)#package to make interactive web
library(leaflet.providers)#allows you to add third party basemaps
library(here)
###Create the map
#Set the content for each point on the map
content_thorndyke <- paste("<b>Thorndyke Bay, Baywater Shellfish - UW</b><br/>
Data: Disolved oxygen, temperature, salinity<br/>
Contact: Craig Norrie")
content_manchester <- paste("<b>Clam Bay, NOAA Manchester - UW</b><br/>
Data: Disolved oxygen, temperature, salinity, pH<br/>
Contact: Craig Norrie")
content_Hood <- paste("<b>Hood Head, Blue Dot Sea Farm - UW</b><br/>
Data: Disolved oxygen, temperature, salinity, pH</b><br/>
Contact: Craig Norrie")
content_Southsound <- paste("<b>South Sound, Chelsea Farms - UW</b><br/>
Data: Disolved oxygen, temperature, salinity<br/>
Contact: Craig Norrie")
#Create the map - in the future set all of the variables in a csv to be read in
aquaculturemap <- leaflet() %>%
addProviderTiles(providers$Esri.OceanBasemap) %>%
addMarkers(lng = -122.738067,lat = 47.808739, popup = content_thorndyke) %>% #Thorndyke Bay
addMarkers(lng = -122.960441,lat = 47.127609, popup = content_Southsound) %>% #Chelsea
addMarkers(lng = -122.545044,lat = 47.573528, popup = content_manchester) %>% #Manchester
addMarkers(lng = -122.612914,lat = 47.884253, popup = content_Hood) #Hood Head
content_Southsound <- paste("<b>South Sound, Chelsea Farms - UW</b><br/>
Data: Disolved oxygen, temperature, salinity<br/>
Contact: Craig Norrie")
#Create the map - in the future set all of the variables in a csv to be read in
aquaculturemap <- leaflet() %>%
addProviderTiles(providers$Esri.OceanBasemap) %>%
addMarkers(lng = -122.738067,lat = 47.808739, popup = content_thorndyke) %>% #Thorndyke Bay
addMarkers(lng = -122.960441,lat = 47.127609, popup = content_Southsound) %>% #Chelsea
addMarkers(lng = -122.545044,lat = 47.573528, popup = content_manchester) %>% #Manchester
addMarkers(lng = -122.612914,lat = 47.884253, popup = content_Hood) #Hood Head
#Create averages
##read in the data
#Thorndyke
thorndat_junjuly <- read.csv(here("Data", "Thorndyke_5_june_to_2_aug_2023.csv"))
thorndat_julyaug <- read.csv(here("Data", "Thorndyke_5_june_to_2_aug_2023.csv"))
#Create averages
##read in the data
allthorndat <- read.csv(here("Data","thorn_Do_Temp"))
#Create averages
##read in the data
allthorndat <- read.csv(here("Data","thorn_Do_Temp.csv"))
allcheldat <- read.csv(here("Data","chelsea_Do_Temp.csv"))
allhooddat <- read.csv(here("Data","hood_Do_Temp.csv"))
allmandat <- read.csv(here("Data","man_Do_Temp.csv"))
#Set the first column as date
allthorndat$datetime <- mdy_hm(allthorndat$Date.Time..GMT.07.00)
str(thornjun_julydat)
#Set the first column as date
allthorndat$datetime <- mdy_hm(allthorndat$Date.Time..GMT.07.00)
allcheldat$datetime  <- mdy_hm(allcheldat$Date.Time..GMT.07.00)
allhooddat$datetime  <- mdy_hm(allhooddat$Date.Time..GMT.07.00)
allmandat$datetime  <- mdy_hm(allmandat$Date.Time..GMT.07.00)
#add a site columin to each of the DFs
allcheldat <- allcheldat %>% mutate(Site = "Chelsea")
allthorndat <- allthorndat %>% mutate(Site = "Thorndyke")
allhooddat <- allhooddat %>% mutate(Site = "Hood_Head")
allmandat <- allmandat %>% mutate(Site = "Manchester")
#add a date column to the two dfs
allcheldat$date <- as.Date(allcheldat$datetime)
allthorndat$date <- as.Date(allthorndat$datetime)
allhooddat$date <- as.Date(allhooddat$datetime)
allmandat$date <- as.Date(allmandat$datetime)
#calculate averages
chelav <- allcheldat %>% group_by(date) %>% summarise(meantemp=mean(Temp_C, na.rm = TRUE), meanDO=mean(DO_mgL, na.rm = TRUE))
thornav <- allthorndat %>% group_by(date) %>% summarise(meantemp=mean(Temp_C, na.rm = TRUE), meanDO=mean(DO_mgL, na.rm = TRUE))
manav <- allmandat %>% group_by(date) %>% summarise(meantemp=mean(Temp_C, na.rm = TRUE), meanDO=mean(DO_mgL, na.rm = TRUE))
hoodav <- allhooddat %>% group_by(date) %>% summarise(meantemp=mean(Temp_C, na.rm = TRUE), meanDO=mean(DO_mgL, na.rm = TRUE))
#Plot DO
averageDO %>%
filter(date > "2023-06-05") %>% filter(date < "2023-10-17") %>%
ggplot(aes(x= date , y = meanDO, colour = Site)) + geom_line(size=1)+
theme_bw()+ylab("Daily Mean Dissolved Oxygen (mg / L)")+xlab("Date")+
scale_color_manual(values=c("#B3C863","#756A8C","#037D64","#CEB08E"))+
theme(axis.title.y=element_text())+
theme(
text = element_text(size = 14))
#Plot DO
avergage_DO_plot <- averageDO %>%
filter(date > "2023-06-05") %>% filter(date < "2023-10-17") %>%
ggplot(aes(x= date , y = meanDO, colour = Site)) + geom_line(size=1)+
theme_bw()+ylab("Daily Mean Dissolved Oxygen (mg / L)")+xlab("Date")+
scale_color_manual(values=c("#B3C863","#756A8C","#037D64","#CEB08E"))+
theme(axis.title.y=element_text())+
theme(
text = element_text(size = 14))
#Only caclulate the days I want
chelav <- chelav %>% filter(date > "2023-06-05")
thornav <- thornav %>% filter(date > "2023-06-05")
hoodav <- hoodav %>% filter(date > "2023-06-05")
manav <- manav %>% filter(date > "2023-06-05")
#calculate cumulative degree days
chelav$degdays <- cumsum(chelav$meantemp)
thornav$degdays <- cumsum(thornav$meantemp)
#Add site to the average dfs
chelav <- chelav %>% mutate(Site = "Chelsea")
thornav <- thornav %>% mutate(Site = "Thorndyke")
manlav <- manav %>% mutate(Site = "Manchester")
hoodav <- hoodav %>% mutate(Site = "Hood_Head")
#merge the two sites temperature data to plot
alldat <- bind_rows(allcheldat, allthorndat, allmandat, allhooddat)
allavdat <- bind_rows(chelav, thornav, hoodav, manav)
#Plot DO
avergage_DO_plot <- averageDO %>%
filter(date > "2023-06-05") %>% filter(date < "2023-10-17") %>%
ggplot(aes(x= date , y = meanDO, colour = Site)) + geom_line(size=1)+
theme_bw()+ylab("Daily Mean Dissolved Oxygen (mg / L)")+xlab("Date")+
scale_color_manual(values=c("#B3C863","#756A8C","#037D64","#CEB08E"))+
theme(axis.title.y=element_text())+
theme(
text = element_text(size = 14))
alldat %>% filter(datetime > "2023-06-05 14:37:00") %>%
ggplot(aes(x = datetime, y = Temp_C, colour = Site))+geom_line()+theme_classic()
dailyaverages <- alldat
alldat$date <- as.Date(alldat$datetime)
averagetemps <- alldat %>% group_by(date, Site) %>% summarise(meantemp=mean(Temp_C, na.rm = TRUE))
averageDO <- alldat %>% group_by(date, Site) %>% summarise(meanDO=mean(DO_mgL, na.rm = TRUE))
#Plot DO
avergage_DO_plot <- averageDO %>%
filter(date > "2023-06-05") %>% filter(date < "2023-10-17") %>%
ggplot(aes(x= date , y = meanDO, colour = Site)) + geom_line(size=1)+
theme_bw()+ylab("Daily Mean Dissolved Oxygen (mg / L)")+xlab("Date")+
scale_color_manual(values=c("#B3C863","#756A8C","#037D64","#CEB08E"))+
theme(axis.title.y=element_text())+
theme(
text = element_text(size = 14))
#Plot DO
avergage_DO_plot <- averageDO %>%
filter(date > "2023-06-05") %>% filter(date < "2023-10-17") %>%
ggplot(aes(x= date , y = meanDO, colour = Site)) + geom_line()+
theme_bw()+ylab("Daily Mean Dissolved Oxygen (mg / L)")+xlab("Date")+
scale_color_manual(values=c("#B3C863","#756A8C","#037D64","#CEB08E"))+
theme(axis.title.y=element_text())+
theme(
text = element_text(size = 14))
avergage_DO_plot
###Create the map
#Read in site info
siteinfo <- read.csv(here("Site_info"))
###Create the map
#Read in site info
siteinfo <- read.csv(here("Site_info.csv"))
#Load the packages required for this
library(tidyverse)
library(leaflet)#package to make interactive web
library(leaflet.providers)#allows you to add third party basemaps
library(here)
library(plotly)
library(lubridate)
###Create the map
#Read in site info
siteinfo <- read.csv(here("Site_info.csv"))
aquaculturemap <- leaflet() %>%
addProviderTiles(providers$Esri.OceanBasemap) %>%
addMarkers(lng = -122.738067,lat = 47.808739, popup = siteinfo$Site_name
)
aquaculturemap
#Create the map - in the future set all of the variables in a csv to be read in
map <- leaflet() %>%
addProviderTiles(providers$Esri.OceanBasemap) %>%
addMarkers(lng = siteinfo$Long ,lat = siteinfo$Lat, popup = siteinfo$Data)
aquaculturemap <- leaflet() %>%
addProviderTiles(providers$Esri.OceanBasemap) %>%
addMarkers(lng = -122.738067,lat = 47.808739, popup = content_thorndyke) #%>% #Thorndyke Bay
addMarkers(lng = -122.960441,lat = 47.127609, popup = content_Southsound) %>% #Chelsea
addMarkers(lng = -122.545044,lat = 47.573528, popup = content_manchester) %>% #Manchester
addMarkers(lng = -122.612914,lat = 47.884253, popup = content_Hood) #Hood Head
aquaculturemap <- leaflet() %>%
addProviderTiles(providers$Esri.OceanBasemap) %>%
addMarkers(lng = -122.738067,lat = 47.808739, popup = content_thorndyke) %>% #Thorndyke Bay
addMarkers(lng = -122.960441,lat = 47.127609, popup = content_Southsound) %>% #Chelsea
addMarkers(lng = -122.545044,lat = 47.573528, popup = content_manchester) %>% #Manchester
addMarkers(lng = -122.612914,lat = 47.884253, popup = content_Hood) #Hood Head
aquaculturemap
library(htmltools)
#Create the map - in the future set all of the variables in a csv to be read in
map <- leaflet() %>%
addProviderTiles(providers$Esri.OceanBasemap) %>%
addMarkers(lng = siteinfo$Long, lat = siteinfo$Lat,
popup = lapply(seq(nrow(siteinfo)), function(i) {
HTML(paste0("<strong>", siteinfo$Site_name[i], "</strong><br>",
"Availble data: ", siteinfo$Data[i], "<br>",
"Organisation: ", siteinfo$Organisation[i], "<br>",
"Contact: ", siteinfo$Contact[i]))
}))
View(siteinfo)
library(here)
---
title: "Hood Head environmental data"
# Chunk 1
#Load the packages required to make a local scale map
library(tidyverse)
library(leaflet)#package to make interactive web
library(leaflet.providers)#allows you to add third party basemaps
library(here)
library(plotly)
library(lubridate)
# Chunk 2
aquaculturemap_hood <- leaflet() %>%
addProviderTiles(providers$Esri.OceanBasemap) %>%
addMarkers(lng = -122.612914,lat = 47.884253, popup = "Hood Head - Blue Dot Sea Farm") #Hood Head
# Chunk 3
aquaculturemap_hood
# Chunk 4
#Sumarise the data
#turn this into a loop that will do this for each of the cvs files that I have in the data folder
hooddat <- read.csv(here("Data", "hood_Do_Temp.csv"))#Read in the data
hooddat$datetime <- mdy_hm(hooddat$Date.Time..GMT.07.00)
#Obtain daily mean data - this should probably be done in a seperate script and this should be saved for the website only
hoodav <- hooddat %>%
mutate(date = as.Date(date, format = "%Y-%m-%d")) %>%
group_by(date) %>%
summarise(meantemp = mean(Temp_C, na.rm = TRUE),
meanDO = mean(DO_mgL, na.rm = TRUE))
# Chunk 5
hood_temp_plot <- hoodav %>%
ggplot(aes(x = date, y = meantemp))+geom_line()+xlab("Date")+ylab("Temperature (째C)")+theme_bw()
# Chunk 6
ggplotly(hood_temp_plot)
# Chunk 7
hood_do_plot <- hoodav %>%
ggplot(aes(x = date, y = meanDO))+geom_line()+xlab("Date")+ylab("Disolved oxygen (mg/L)")+theme_bw()
ggplotly(hood_do_plot)
# Chunk 8
ggplotly(hood_do_plot)
hood_do_plot <- hoodav %>%
ggplot(aes(x = date, y = meanDO))+geom_line()+xlab("Date")+ylab("Disolved oxygen (mg/L)")+theme_bw()+geom_blank()
ggplotly(hood_do_plot)
ggplot(aes(x = date, y = meanDO))+geom_line()+xlab("Date")+ylab("Disolved oxygen (mg/L)")+theme_bw()+geom_blank()
ggplot(aes(x = date, y = meanDO))+geom_line()+xlab("Date")+ylab("Disolved oxygen (mg/L)")+theme_bw()
aquaculturemap_hood <- leaflet() %>%
addProviderTiles(providers$Esri.OceanBasemap) %>%
addMarkers(lng = -122.612914,lat = 47.884253, popup = "Hood Head - Blue Dot Sea Farm") #Hood Head
aquaculturemap_hood
#Sumarise the data
#turn this into a loop that will do this for each of the cvs files that I have in the data folder
hooddat <- read.csv(here("Data", "hood_Do_Temp.csv"))#Read in the data
hooddat$datetime <- mdy_hm(hooddat$Date.Time..GMT.07.00)
#Obtain daily mean data - this should probably be done in a seperate script and this should be saved for the website only
hoodav <- hooddat %>%
mutate(date = as.Date(date, format = "%Y-%m-%d")) %>%
group_by(date) %>%
summarise(meantemp = mean(Temp_C, na.rm = TRUE),
meanDO = mean(DO_mgL, na.rm = TRUE))
hood_temp_plot <- hoodav %>%
ggplot(aes(x = date, y = meantemp))+geom_line()+xlab("Date")+ylab("Temperature (째C)")+theme_bw()
ggplotly(hood_temp_plot)
hood_do_plot <- hoodav %>%
ggplot(aes(x = date, y = meanDO))+geom_line()+xlab("Date")+ylab("Disolved oxygen (mg/L)")+theme_bw()#+geom_blank()
ggplotly(hood_do_plot)
ggplotly(hood_do_plot)
#Sumarise the data
#turn this into a loop that will do this for each of the cvs files that I have in the data folder
hooddat <- read.csv(here("Data", "hood_Do_Temp.csv"))#Read in the data
hooddat$datetime <- mdy_hm(hooddat$Date.Time..GMT.07.00)
#Obtain daily mean data - this should probably be done in a seperate script and this should be saved for the website only
hoodav <- hooddat %>%
mutate(date = as.Date(date, format = "%Y-%m-%d")) %>%
group_by(date) %>%
summarise(meantemp = mean(Temp_C, na.rm = TRUE),
meanDO = mean(DO_mgL, na.rm = TRUE))
hood_temp_plot <- hoodav %>%
ggplot(aes(x = date, y = meantemp))+geom_line()+xlab("Date")+ylab("Temperature (째C)")+theme_bw()
ggplotly(hood_temp_plot)
