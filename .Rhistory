install.packages("plotly")
install.packages(c("tidyverse", "leaflet", "here", "plotly", "leaflet.providers"))
library(tidyverse)
install.packages("tidyverse")
#Load the packages required to make a local scale map
library(tidyverse)
#Load the packages required for this
library(tidyverse)
install.packages("tidyverse")
library(leaflet)#package to make interactive web
library(leaflet.providers)#allows you to add third party basemaps
#Load the packages required for this
library(tidyverse)
install.packages("tidyverse")
#Load the packages required for this
library(tidyverse)
install.packages("tidyverse")
install.packages("Rtools")
install.packages("tidyverse")
#Load the packages required to make a local scale map
library(tidyverse)
#Load the packages required to make a local scale map
library(tidyverse)
install.packages("tidyverse")
#Load the packages required to make a local scale map
library(tidyverse)
install.packages("gtable")
#Load the packages required to make a local scale map
library(tidyverse)
install.packages("munsell")
#Load the packages required to make a local scale map
library(tidyverse)
install.packages("utf8")
#Load the packages required to make a local scale map
library(tidyverse)
install.packages("tzdb")
#Load the packages required to make a local scale map
library(tidyverse)
library(leaflet)#package to make interactive web
library(leaflet.providers)#allows you to add third party basemaps
library(here)
library(plotly)
ggplotly(Thorn_do_plot)
#turn this into a loop that will do this for each of the cvs files that I have in the data folder
thorndat <- read.csv(here("Data", "thorn_Do_Temp.csv"))#Read in the data
thorndat$datetime <- mdy_hm(thorndat$Date.Time..GMT.07.00)
#Obtain daily mean data - this should probably be done in a seperate script and this should be saved for the website only
thornav <- thorndat %>%
mutate(date = as.Date(date, format = "%Y-%m-%d")) %>%
group_by(date) %>%
summarise(meantemp = mean(Temp_C, na.rm = TRUE),
meanDO = mean(DO_mgL, na.rm = TRUE))
#Obtain daily mean data - this should probably be done in a seperate script and this should be saved for the website only
thornav <- thorndat %>%
mutate(date = as.Date(date, format = "%Y-%m-%d")) %>%
group_by(date) %>%
summarise(meantemp = mean(Temp_C, na.rm = TRUE),
meanDO = mean(DO_mgL, na.rm = TRUE))
thorndat$datetime <- mdy_hm(thorndat$Date.Time..GMT.07.00)
install.packages("lubridate")
thorndat$datetime <- mdy_hm(thorndat$Date.Time..GMT.07.00)
mdy_hm
??mdy_hm
library(lubridate)
thorndat$datetime <- mdy_hm(thorndat$Date.Time..GMT.07.00)
#Obtain daily mean data - this should probably be done in a seperate script and this should be saved for the website only
thornav <- thorndat %>%
mutate(date = as.Date(date, format = "%Y-%m-%d")) %>%
group_by(date) %>%
summarise(meantemp = mean(Temp_C, na.rm = TRUE),
meanDO = mean(DO_mgL, na.rm = TRUE))
Thorn_do_plot <- thornav %>%
ggplot(aes(x = date, y = meanDO))+geom_line()+xlab("Date")+ylab("Disolved oxygen (mg/L)")
Thorn_do_plot
ggplotly(Thorn_do_plot)
ggplotly(Thorn_temp_plot)
#Load the packages required to make a local scale map
library(tidyverse)
library(leaflet)#package to make interactive web
library(leaflet.providers)#allows you to add third party basemaps
library(here)
library(plotly)
library(lubridate)
library(tidyverse)
library(leaflet)#package to make interactive web
library(leaflet.providers)#allows you to add third party basemaps
library(here)
library(plotly)
library(lubridate)
#Sumarise the data
#turn this into a loop that will do this for each of the cvs files that I have in the data folder
cheldat <- read.csv(here("Data", "chel_Do_Temp.csv"))#Read in the data
#Sumarise the data
#turn this into a loop that will do this for each of the cvs files that I have in the data folder
cheldat <- read.csv(here("Data", "chelsea_Do_Temp.csv"))#Read in the data
cheldat$datetime <- mdy_hm(cheldat$Date.Time..GMT.07.00)
#Obtain daily mean data - this should probably be done in a seperate script and this should be saved for the website only
chelav <- cheldat %>%
mutate(date = as.Date(date, format = "%Y-%m-%d")) %>%
group_by(date) %>%
summarise(meantemp = mean(Temp_C, na.rm = TRUE),
meanDO = mean(DO_mgL, na.rm = TRUE))
#Sumarise the data
#turn this into a loop that will do this for each of the cvs files that I have in the data folder
cheldat <- read.csv(here("Data", "chelsea_Do_Temp.csv"))#Read in the data
cheldat$datetime <- mdy_hm(cheldat$Date.Time..GMT.07.00)
#Obtain daily mean data - this should probably be done in a seperate script and this should be saved for the website only
chelav <- cheldat %>%
mutate(date = as.Date(date, format = "%Y-%m-%d")) %>%
group_by(date) %>%
summarise(meantemp = mean(Temp_C, na.rm = TRUE),
meanDO = mean(DO_mgL, na.rm = TRUE))
chel_temp_plot <- chelav %>%
ggplot(aes(x = date, y = meantemp))+geom_line()+xlab("Date")+ylab("Temperature (°C)")+theme_bw()
ggplotly(chel_temp_plot)
copy NUL .nojekyll
#Create averages
##read in the data
#Thorndyke
thorndat_junjuly <- read.csv(here("Data", "Thorndyke_5_june_to_2_aug_2023.csv"))
library(here)
#Load the packages required for this
library(tidyverse)
library(leaflet)#package to make interactive web
library(leaflet.providers)#allows you to add third party basemaps
library(here)
###Create the map
#Set the content for each point on the map
content_thorndyke <- paste("<b>Thorndyke Bay, Baywater Shellfish - UW</b><br/>
Data: Disolved oxygen, temperature, salinity<br/>
Contact: Craig Norrie")
content_manchester <- paste("<b>Clam Bay, NOAA Manchester - UW</b><br/>
Data: Disolved oxygen, temperature, salinity, pH<br/>
Contact: Craig Norrie")
content_Hood <- paste("<b>Hood Head, Blue Dot Sea Farm - UW</b><br/>
Data: Disolved oxygen, temperature, salinity, pH</b><br/>
Contact: Craig Norrie")
content_Southsound <- paste("<b>South Sound, Chelsea Farms - UW</b><br/>
Data: Disolved oxygen, temperature, salinity<br/>
Contact: Craig Norrie")
#Create the map - in the future set all of the variables in a csv to be read in
aquaculturemap <- leaflet() %>%
addProviderTiles(providers$Esri.OceanBasemap) %>%
addMarkers(lng = -122.738067,lat = 47.808739, popup = content_thorndyke) %>% #Thorndyke Bay
addMarkers(lng = -122.960441,lat = 47.127609, popup = content_Southsound) %>% #Chelsea
addMarkers(lng = -122.545044,lat = 47.573528, popup = content_manchester) %>% #Manchester
addMarkers(lng = -122.612914,lat = 47.884253, popup = content_Hood) #Hood Head
content_Southsound <- paste("<b>South Sound, Chelsea Farms - UW</b><br/>
Data: Disolved oxygen, temperature, salinity<br/>
Contact: Craig Norrie")
#Create the map - in the future set all of the variables in a csv to be read in
aquaculturemap <- leaflet() %>%
addProviderTiles(providers$Esri.OceanBasemap) %>%
addMarkers(lng = -122.738067,lat = 47.808739, popup = content_thorndyke) %>% #Thorndyke Bay
addMarkers(lng = -122.960441,lat = 47.127609, popup = content_Southsound) %>% #Chelsea
addMarkers(lng = -122.545044,lat = 47.573528, popup = content_manchester) %>% #Manchester
addMarkers(lng = -122.612914,lat = 47.884253, popup = content_Hood) #Hood Head
#Create averages
##read in the data
#Thorndyke
thorndat_junjuly <- read.csv(here("Data", "Thorndyke_5_june_to_2_aug_2023.csv"))
thorndat_julyaug <- read.csv(here("Data", "Thorndyke_5_june_to_2_aug_2023.csv"))
#Create averages
##read in the data
allthorndat <- read.csv(here("Data","thorn_Do_Temp"))
#Create averages
##read in the data
allthorndat <- read.csv(here("Data","thorn_Do_Temp.csv"))
allcheldat <- read.csv(here("Data","chelsea_Do_Temp.csv"))
allhooddat <- read.csv(here("Data","hood_Do_Temp.csv"))
allmandat <- read.csv(here("Data","man_Do_Temp.csv"))
#Set the first column as date
allthorndat$datetime <- mdy_hm(allthorndat$Date.Time..GMT.07.00)
str(thornjun_julydat)
#Set the first column as date
allthorndat$datetime <- mdy_hm(allthorndat$Date.Time..GMT.07.00)
allcheldat$datetime  <- mdy_hm(allcheldat$Date.Time..GMT.07.00)
allhooddat$datetime  <- mdy_hm(allhooddat$Date.Time..GMT.07.00)
allmandat$datetime  <- mdy_hm(allmandat$Date.Time..GMT.07.00)
#add a site columin to each of the DFs
allcheldat <- allcheldat %>% mutate(Site = "Chelsea")
allthorndat <- allthorndat %>% mutate(Site = "Thorndyke")
allhooddat <- allhooddat %>% mutate(Site = "Hood_Head")
allmandat <- allmandat %>% mutate(Site = "Manchester")
#add a date column to the two dfs
allcheldat$date <- as.Date(allcheldat$datetime)
allthorndat$date <- as.Date(allthorndat$datetime)
allhooddat$date <- as.Date(allhooddat$datetime)
allmandat$date <- as.Date(allmandat$datetime)
#calculate averages
chelav <- allcheldat %>% group_by(date) %>% summarise(meantemp=mean(Temp_C, na.rm = TRUE), meanDO=mean(DO_mgL, na.rm = TRUE))
thornav <- allthorndat %>% group_by(date) %>% summarise(meantemp=mean(Temp_C, na.rm = TRUE), meanDO=mean(DO_mgL, na.rm = TRUE))
manav <- allmandat %>% group_by(date) %>% summarise(meantemp=mean(Temp_C, na.rm = TRUE), meanDO=mean(DO_mgL, na.rm = TRUE))
hoodav <- allhooddat %>% group_by(date) %>% summarise(meantemp=mean(Temp_C, na.rm = TRUE), meanDO=mean(DO_mgL, na.rm = TRUE))
#Plot DO
averageDO %>%
filter(date > "2023-06-05") %>% filter(date < "2023-10-17") %>%
ggplot(aes(x= date , y = meanDO, colour = Site)) + geom_line(size=1)+
theme_bw()+ylab("Daily Mean Dissolved Oxygen (mg / L)")+xlab("Date")+
scale_color_manual(values=c("#B3C863","#756A8C","#037D64","#CEB08E"))+
theme(axis.title.y=element_text())+
theme(
text = element_text(size = 14))
#Plot DO
avergage_DO_plot <- averageDO %>%
filter(date > "2023-06-05") %>% filter(date < "2023-10-17") %>%
ggplot(aes(x= date , y = meanDO, colour = Site)) + geom_line(size=1)+
theme_bw()+ylab("Daily Mean Dissolved Oxygen (mg / L)")+xlab("Date")+
scale_color_manual(values=c("#B3C863","#756A8C","#037D64","#CEB08E"))+
theme(axis.title.y=element_text())+
theme(
text = element_text(size = 14))
#Only caclulate the days I want
chelav <- chelav %>% filter(date > "2023-06-05")
thornav <- thornav %>% filter(date > "2023-06-05")
hoodav <- hoodav %>% filter(date > "2023-06-05")
manav <- manav %>% filter(date > "2023-06-05")
#calculate cumulative degree days
chelav$degdays <- cumsum(chelav$meantemp)
thornav$degdays <- cumsum(thornav$meantemp)
#Add site to the average dfs
chelav <- chelav %>% mutate(Site = "Chelsea")
thornav <- thornav %>% mutate(Site = "Thorndyke")
manlav <- manav %>% mutate(Site = "Manchester")
hoodav <- hoodav %>% mutate(Site = "Hood_Head")
#merge the two sites temperature data to plot
alldat <- bind_rows(allcheldat, allthorndat, allmandat, allhooddat)
allavdat <- bind_rows(chelav, thornav, hoodav, manav)
#Plot DO
avergage_DO_plot <- averageDO %>%
filter(date > "2023-06-05") %>% filter(date < "2023-10-17") %>%
ggplot(aes(x= date , y = meanDO, colour = Site)) + geom_line(size=1)+
theme_bw()+ylab("Daily Mean Dissolved Oxygen (mg / L)")+xlab("Date")+
scale_color_manual(values=c("#B3C863","#756A8C","#037D64","#CEB08E"))+
theme(axis.title.y=element_text())+
theme(
text = element_text(size = 14))
alldat %>% filter(datetime > "2023-06-05 14:37:00") %>%
ggplot(aes(x = datetime, y = Temp_C, colour = Site))+geom_line()+theme_classic()
dailyaverages <- alldat
alldat$date <- as.Date(alldat$datetime)
averagetemps <- alldat %>% group_by(date, Site) %>% summarise(meantemp=mean(Temp_C, na.rm = TRUE))
averageDO <- alldat %>% group_by(date, Site) %>% summarise(meanDO=mean(DO_mgL, na.rm = TRUE))
#Plot DO
avergage_DO_plot <- averageDO %>%
filter(date > "2023-06-05") %>% filter(date < "2023-10-17") %>%
ggplot(aes(x= date , y = meanDO, colour = Site)) + geom_line(size=1)+
theme_bw()+ylab("Daily Mean Dissolved Oxygen (mg / L)")+xlab("Date")+
scale_color_manual(values=c("#B3C863","#756A8C","#037D64","#CEB08E"))+
theme(axis.title.y=element_text())+
theme(
text = element_text(size = 14))
#Plot DO
avergage_DO_plot <- averageDO %>%
filter(date > "2023-06-05") %>% filter(date < "2023-10-17") %>%
ggplot(aes(x= date , y = meanDO, colour = Site)) + geom_line()+
theme_bw()+ylab("Daily Mean Dissolved Oxygen (mg / L)")+xlab("Date")+
scale_color_manual(values=c("#B3C863","#756A8C","#037D64","#CEB08E"))+
theme(axis.title.y=element_text())+
theme(
text = element_text(size = 14))
avergage_DO_plot
###Create the map
#Read in site info
siteinfo <- read.csv(here("Site_info"))
###Create the map
#Read in site info
siteinfo <- read.csv(here("Site_info.csv"))
#Load the packages required for this
library(tidyverse)
library(leaflet)#package to make interactive web
library(leaflet.providers)#allows you to add third party basemaps
library(here)
library(plotly)
library(lubridate)
###Create the map
#Read in site info
siteinfo <- read.csv(here("Site_info.csv"))
aquaculturemap <- leaflet() %>%
addProviderTiles(providers$Esri.OceanBasemap) %>%
addMarkers(lng = -122.738067,lat = 47.808739, popup = siteinfo$Site_name
)
aquaculturemap
#Create the map - in the future set all of the variables in a csv to be read in
map <- leaflet() %>%
addProviderTiles(providers$Esri.OceanBasemap) %>%
addMarkers(lng = siteinfo$Long ,lat = siteinfo$Lat, popup = siteinfo$Data)
aquaculturemap <- leaflet() %>%
addProviderTiles(providers$Esri.OceanBasemap) %>%
addMarkers(lng = -122.738067,lat = 47.808739, popup = content_thorndyke) #%>% #Thorndyke Bay
addMarkers(lng = -122.960441,lat = 47.127609, popup = content_Southsound) %>% #Chelsea
addMarkers(lng = -122.545044,lat = 47.573528, popup = content_manchester) %>% #Manchester
addMarkers(lng = -122.612914,lat = 47.884253, popup = content_Hood) #Hood Head
aquaculturemap <- leaflet() %>%
addProviderTiles(providers$Esri.OceanBasemap) %>%
addMarkers(lng = -122.738067,lat = 47.808739, popup = content_thorndyke) %>% #Thorndyke Bay
addMarkers(lng = -122.960441,lat = 47.127609, popup = content_Southsound) %>% #Chelsea
addMarkers(lng = -122.545044,lat = 47.573528, popup = content_manchester) %>% #Manchester
addMarkers(lng = -122.612914,lat = 47.884253, popup = content_Hood) #Hood Head
aquaculturemap
library(htmltools)
#Create the map - in the future set all of the variables in a csv to be read in
map <- leaflet() %>%
addProviderTiles(providers$Esri.OceanBasemap) %>%
addMarkers(lng = siteinfo$Long, lat = siteinfo$Lat,
popup = lapply(seq(nrow(siteinfo)), function(i) {
HTML(paste0("<strong>", siteinfo$Site_name[i], "</strong><br>",
"Availble data: ", siteinfo$Data[i], "<br>",
"Organisation: ", siteinfo$Organisation[i], "<br>",
"Contact: ", siteinfo$Contact[i]))
}))
View(siteinfo)
library(here)
---
title: "Hood Head environmental data"
# Chunk 1
#Load the packages required to make a local scale map
library(tidyverse)
library(leaflet)#package to make interactive web
library(leaflet.providers)#allows you to add third party basemaps
library(here)
library(plotly)
library(lubridate)
# Chunk 2
aquaculturemap_hood <- leaflet() %>%
addProviderTiles(providers$Esri.OceanBasemap) %>%
addMarkers(lng = -122.612914,lat = 47.884253, popup = "Hood Head - Blue Dot Sea Farm") #Hood Head
# Chunk 3
aquaculturemap_hood
# Chunk 4
#Sumarise the data
#turn this into a loop that will do this for each of the cvs files that I have in the data folder
hooddat <- read.csv(here("Data", "hood_Do_Temp.csv"))#Read in the data
hooddat$datetime <- mdy_hm(hooddat$Date.Time..GMT.07.00)
#Obtain daily mean data - this should probably be done in a seperate script and this should be saved for the website only
hoodav <- hooddat %>%
mutate(date = as.Date(date, format = "%Y-%m-%d")) %>%
group_by(date) %>%
summarise(meantemp = mean(Temp_C, na.rm = TRUE),
meanDO = mean(DO_mgL, na.rm = TRUE))
# Chunk 5
hood_temp_plot <- hoodav %>%
ggplot(aes(x = date, y = meantemp))+geom_line()+xlab("Date")+ylab("Temperature (°C)")+theme_bw()
# Chunk 6
ggplotly(hood_temp_plot)
# Chunk 7
hood_do_plot <- hoodav %>%
ggplot(aes(x = date, y = meanDO))+geom_line()+xlab("Date")+ylab("Disolved oxygen (mg/L)")+theme_bw()
ggplotly(hood_do_plot)
# Chunk 8
ggplotly(hood_do_plot)
hood_do_plot <- hoodav %>%
ggplot(aes(x = date, y = meanDO))+geom_line()+xlab("Date")+ylab("Disolved oxygen (mg/L)")+theme_bw()+geom_blank()
ggplotly(hood_do_plot)
ggplot(aes(x = date, y = meanDO))+geom_line()+xlab("Date")+ylab("Disolved oxygen (mg/L)")+theme_bw()+geom_blank()
ggplot(aes(x = date, y = meanDO))+geom_line()+xlab("Date")+ylab("Disolved oxygen (mg/L)")+theme_bw()
aquaculturemap_hood <- leaflet() %>%
addProviderTiles(providers$Esri.OceanBasemap) %>%
addMarkers(lng = -122.612914,lat = 47.884253, popup = "Hood Head - Blue Dot Sea Farm") #Hood Head
aquaculturemap_hood
#Sumarise the data
#turn this into a loop that will do this for each of the cvs files that I have in the data folder
hooddat <- read.csv(here("Data", "hood_Do_Temp.csv"))#Read in the data
hooddat$datetime <- mdy_hm(hooddat$Date.Time..GMT.07.00)
#Obtain daily mean data - this should probably be done in a seperate script and this should be saved for the website only
hoodav <- hooddat %>%
mutate(date = as.Date(date, format = "%Y-%m-%d")) %>%
group_by(date) %>%
summarise(meantemp = mean(Temp_C, na.rm = TRUE),
meanDO = mean(DO_mgL, na.rm = TRUE))
hood_temp_plot <- hoodav %>%
ggplot(aes(x = date, y = meantemp))+geom_line()+xlab("Date")+ylab("Temperature (°C)")+theme_bw()
ggplotly(hood_temp_plot)
hood_do_plot <- hoodav %>%
ggplot(aes(x = date, y = meanDO))+geom_line()+xlab("Date")+ylab("Disolved oxygen (mg/L)")+theme_bw()#+geom_blank()
ggplotly(hood_do_plot)
ggplotly(hood_do_plot)
#Sumarise the data
#turn this into a loop that will do this for each of the cvs files that I have in the data folder
hooddat <- read.csv(here("Data", "hood_Do_Temp.csv"))#Read in the data
hooddat$datetime <- mdy_hm(hooddat$Date.Time..GMT.07.00)
#Obtain daily mean data - this should probably be done in a seperate script and this should be saved for the website only
hoodav <- hooddat %>%
mutate(date = as.Date(date, format = "%Y-%m-%d")) %>%
group_by(date) %>%
summarise(meantemp = mean(Temp_C, na.rm = TRUE),
meanDO = mean(DO_mgL, na.rm = TRUE))
hood_temp_plot <- hoodav %>%
ggplot(aes(x = date, y = meantemp))+geom_line()+xlab("Date")+ylab("Temperature (°C)")+theme_bw()
ggplotly(hood_temp_plot)
library(tidyverse)
library(leaflet)#package to make interactive web
library(leaflet.providers)#allows you to add third party basemaps
library(here)
library(plotly)
library(lubridate)
mandat <- read.csv(here("Data", "man_Do_Temp.csv"))#Read in the data
mandat$datetime <- mdy_hm(mandat$Date.Time..GMT.07.00)
#Obtain daily mean data - this should probably be done in a seperate script and this should be saved for the website only
manav <- mandat %>%
mutate(date = as.Date(date, format = "%Y-%m-%d")) %>%
group_by(date) %>%
summarise(meantemp = mean(Temp_C, na.rm = TRUE),
meanDO = mean(DO_mgL, na.rm = TRUE))
man_temp_plot <- manav %>%
ggplot(aes(x = date, y = meantemp))+geom_line()+xlab("Date")+ylab("Temperature (°C)")+theme_bw()
man_temp_plot
mandat <- read.csv(here("Data", "man_Do_Temp.csv"))#Read in the data
mandat$datetime <- mdy_hm(mandat$Date.Time..GMT.07.00)
#Obtain daily mean data - this should probably be done in a seperate script and this should be saved for the website only
manav <- mandat %>%
mutate(date = as.Date(date, format = "%Y-%m-%d")) %>%
group_by(date) %>%
summarise(meantemp = mean(Temp_C, na.rm = TRUE),
meanDO = mean(DO_mgL, na.rm = TRUE))
man_temp_plot <- manav %>%
ggplot(aes(x = date, y = meantemp))+geom_line()+xlab("Date")+ylab("Temperature (°C)")+theme_bw()
man_temp_plot
#Sumarise the data
#turn this into a loop that will do this for each of the cvs files that I have in the data folder
mandat <- read.csv(here("Data", "man_Do_Temp.csv"))#Read in the data
mandat$datetime <- mdy_hm(mandat$Date.Time..GMT.07.00)
#Obtain daily mean data - this should probably be done in a seperate script and this should be saved for the website only
manav <- mandat %>%
mutate(date = as.Date(date, format = "%Y-%m-%d")) %>%
group_by(date) %>%
summarise(meantemp = mean(Temp_C, na.rm = TRUE),
meanDO = mean(DO_mgL, na.rm = TRUE))
man_temp_plot <- manav %>%
ggplot(aes(x = date, y = meantemp))+geom_line()+xlab("Date")+ylab("Temperature (°C)")+theme_bw()
man_temp_plot
mandat <- read.csv(here("Data", "man_Do_Temp.csv"))#Read in the data
mandat$datetime <- mdy_hm(mandat$Date.Time..GMT.07.00)
#Obtain daily mean data - this should probably be done in a seperate script and this should be saved for the website only
manav <- mandat %>%
mutate(date = as.Date(date, format = "%Y-%m-%d")) %>%
group_by(date) %>%
summarise(meantemp = mean(Temp_C, na.rm = TRUE),
meanDO = mean(DO_mgL, na.rm = TRUE))
man_temp_plot <- manav %>%
ggplot(aes(x = date, y = meantemp))+geom_line()+xlab("Date")+ylab("Temperature (°C)")+theme_bw()
man_temp_plot
ggplotly(man_do_plot)
ggplotly(man_temp_plot)
ggplotly(man_temp_plot)
man_temp_plot
---
title: "Chelsea environmental data"
# Chunk 1
#Load the packages required to make a local scale map
library(tidyverse)
library(leaflet)#package to make interactive web
library(leaflet.providers)#allows you to add third party basemaps
library(here)
library(plotly)
library(lubridate)
# Chunk 2
aquaculturemap_chel <- leaflet() %>%
addProviderTiles(providers$Esri.OceanBasemap) %>%
addMarkers(lng = -122.960441,lat = 47.127609, popup = "South Sound - Chelsea Farms")
# Chunk 3
aquaculturemap_chel
# Chunk 4
#Sumarise the data
#turn this into a loop that will do this for each of the cvs files that I have in the data folder
cheldat <- read.csv(here("Data", "chelsea_Do_Temp.csv"))#Read in the data
cheldat$datetime <- mdy_hm(cheldat$Date.Time..GMT.07.00)
#Obtain daily mean data - this should probably be done in a separate script and this should be saved for the website only
chelav <- cheldat %>%
mutate(date = as.Date(date, format = "%Y-%m-%d")) %>%
group_by(date) %>%
summarise(meantemp = mean(Temp_C, na.rm = TRUE),
meanDO = mean(DO_mgL, na.rm = TRUE))
# Chunk 5
chel_temp_plot <- chelav %>%
ggplot(aes(x = date, y = meantemp))+geom_line()+xlab("Date")+ylab("Temperature (°C)")+theme_bw()
# Chunk 6
ggplotly(chel_temp_plot)
# Chunk 7
chel_do_plot <- chelav %>%
ggplot(aes(x = date, y = meanDO))+geom_line()+xlab("Date")+ylab("Disolved oxygen (mg/L)")+theme_bw()
ggplotly(chel_do_plot)
# Chunk 8
ggplotly(chel_do_plot)
###
experiment_name <- "Multistressor_2024"
# Load packages ------------------------------------------------------------
library(tidyverse)
library(gridExtra)
library(lubridate)
library(respR) # I had to install this from github devtools::install_github("januarharianto/respR")
library(here)
# Read in all the data ----------------------------------------------------
rawfiles <- list.files(here("Raw_data"),
all.files = FALSE) #Creates a vector with all the filenames from respirometry that are stored in the "raw_data" folder.
respdat_all <- data.frame()#Creates an empty data frame to store respirometry data
#The for loop here will read in each of the data files
for(i in 1:length(rawfiles)){
tempdat <- read.csv(file = here("Raw_data", rawfiles[i]), skip = 1)
tempdat <- slice(tempdat, 1:(n() - 2))#removes the last two rows becasue it is junk
tempdat$run <- i#add a new column based on the number sheet that it is. i.e. run 1, run 2. **important that sheets are named in order of resplog**
respdat_all <- rbind(respdat_all, tempdat)#used plyr::rbind instead of dplyr::bind_rows because this deals with duplicate column names
rm(tempdat)
}
respdat_all
# Read in all the data ----------------------------------------------------
rawfiles <- list.files(here("Raw_data"),
all.files = FALSE) #Creates a vector with all the filenames from respirometry that are stored in the "raw_data" folder.
rawfiles
###
experiment_name <- "Multistressor_2024"
here()
